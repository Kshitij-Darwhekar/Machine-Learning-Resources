# -*- coding: utf-8 -*-
"""Hierarchical Clustering FDP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wSLfWXMXzDuPNoOE-Z1yWKKvRPyOCuIt
"""

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from google.colab import files
uploaded = files.upload()

# Importing the dataset
dataset = pd.read_csv('Mall_Customers.csv')
dataset

# Mounting Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Importing the dataset
dataset = pd.read_csv('/content/drive/My Drive/FDP/Mall_Customers.csv')

print(dataset)

#Print Total number of Rows & columns in dataset
print(dataset.shape)

#Print Information about data
dataset.info()

types = dataset.dtypes
print(types)

#Count total number of classes in Data
class_counts = dataset.groupby('Genre').size()
print(class_counts)

from matplotlib import pyplot
dataset.hist()
pyplot.show()

dataset.plot(kind='density' ,subplots=True, layout=(3,3), sharex=False)
pyplot.show()

# Extracting features of dataset

X = dataset.iloc[:, [3, 4]].values

print(X)

# Using the dendrogram to find the optimal number of clusters
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method = 'single'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

# Using the dendrogram to find the optimal number of clusters
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method = 'complete'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

# Using the dendrogram to find the optimal number of clusters
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method = 'average'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

# Using the dendrogram to find the optimal number of clusters
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

import scipy.cluster.hierarchy as sch
plt.figure(figsize=(10, 7))  
plt.title("Dendrograms")  
dend = sch.dendrogram(sch.linkage(X, method='ward'))
plt.axhline(y=200, color='r', linestyle='--')

from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
import numpy as np

Z = linkage(X, 'ward')

from scipy.cluster.hierarchy import cophenet
from scipy.spatial.distance import pdist

c, coph_dists = cophenet(Z, pdist(X))
c

A = linkage(X, 'average')

from scipy.cluster.hierarchy import cophenet
from scipy.spatial.distance import pdist

c, coph_dists = cophenet(A, pdist(X))
c

C = linkage(X, 'complete')

from scipy.cluster.hierarchy import cophenet
from scipy.spatial.distance import pdist

c, coph_dists = cophenet(C, pdist(X))
c

Ce = linkage(X, 'centroid')

from scipy.cluster.hierarchy import cophenet
from scipy.spatial.distance import pdist

c, coph_dists = cophenet(Ce, pdist(X))
c

S = linkage(X, 'single')

from scipy.cluster.hierarchy import cophenet
from scipy.spatial.distance import pdist

c, coph_dists = cophenet(S, pdist(X))
c

Z[0]

dataset.isnull().sum()

# Fitting Hierarchical Clustering to the dataset
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'single')
y_hc = hc.fit_predict(X)

print(y_hc)

# Visualising the clusters
plt.scatter(X[:,0], X[:,1], s = 100, c = 'black', label = 'Data Distribution')
plt.title('Customer Distribution before clustering')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

# Visualising the clusters
plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

# Visualising the clusters
plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careless')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'standard')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Target')
plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Careful')
plt.scatter(X[y_hc== 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

frame = pd.DataFrame(X)
frame['cluster'] = y_hc
frame['cluster'].value_counts()

Annual_Income =  39#@param {type:"number"}
Spending_Score =  91#@param {type:"number"}
Annual_Income1 =  34#@param {type:"number"}
Spending_Score1 =  19#@param {type:"number"}
Annual_Income2 = 34 #@param {type:"number"}
Spending_Score2 =  65#@param {type:"number"}
Annual_Income3 =  45#@param {type:"number"}
Spending_Score3 =  56#@param {type:"number"}
Annual_Income4 =  56#@param {type:"number"}
Spending_Score4 =  87#@param {type:"number"}
predict= hc.fit_predict([[ Annual_Income,Spending_Score ],[ Annual_Income1,Spending_Score1 ], [ Annual_Income2,Spending_Score2 ], [ Annual_Income3,Spending_Score3], [ Annual_Income4,Spending_Score4 ]])
print(predict)

# Fitting Hierarchical Clustering to the dataset
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(X)

print(y_hc)

# Visualising the clusters
plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careless')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'standard')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Target')
plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Careful')
plt.scatter(X[y_hc== 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

Annual_Income =  39#@param {type:"number"}
Spending_Score =  91#@param {type:"number"}
Annual_Income1 =  34#@param {type:"number"}
Spending_Score1 =  19#@param {type:"number"}
Annual_Income2 = 34 #@param {type:"number"}
Spending_Score2 =  65#@param {type:"number"}
Annual_Income3 =  45#@param {type:"number"}
Spending_Score3 =  56#@param {type:"number"}
Annual_Income4 =  56#@param {type:"number"}
Spending_Score4 =  21#@param {type:"number"}
predict= hc.fit_predict([[ Annual_Income,Spending_Score ],[ Annual_Income1,Spending_Score1 ], [ Annual_Income2,Spending_Score2 ], [ Annual_Income3,Spending_Score3], [ Annual_Income4,Spending_Score4 ]])
print(predict)

# Fitting Hierarchical Clustering to the dataset
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(X)

print(y_hc)

# Visualising the clusters
plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careless')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'standard')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Target')
plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Careful')
plt.scatter(X[y_hc== 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

Annual_Income =  39#@param {type:"number"}
Spending_Score =  91#@param {type:"number"}
Annual_Income1 =  34#@param {type:"number"}
Spending_Score1 =  19#@param {type:"number"}
Annual_Income2 = 34 #@param {type:"number"}
Spending_Score2 =  65#@param {type:"number"}
Annual_Income3 =  45#@param {type:"number"}
Spending_Score3 =  56#@param {type:"number"}
Annual_Income4 =  56#@param {type:"number"}
Spending_Score4 =  21#@param {type:"number"}
predict= hc.fit_predict([[ Annual_Income,Spending_Score ],[ Annual_Income1,Spending_Score1 ], [ Annual_Income2,Spending_Score2 ], [ Annual_Income3,Spending_Score3], [ Annual_Income4,Spending_Score4 ]])
print(predict)

# Fitting Hierarchical Clustering to the dataset
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage='average')
y_hc = hc.fit_predict(X)

print(y_hc)

# Visualising the clusters
plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careless')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'standard')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Target')
plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Careful')
plt.scatter(X[y_hc== 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

Annual_Income =  39#@param {type:"number"}
Spending_Score =  91#@param {type:"number"}
Annual_Income1 =  34#@param {type:"number"}
Spending_Score1 =  19#@param {type:"number"}
Annual_Income2 = 34 #@param {type:"number"}
Spending_Score2 =  65#@param {type:"number"}
Annual_Income3 =  45#@param {type:"number"}
Spending_Score3 =  56#@param {type:"number"}
Annual_Income4 =  56#@param {type:"number"}
Spending_Score4 =  21#@param {type:"number"}
predict= hc.fit_predict([[ Annual_Income,Spending_Score ],[ Annual_Income1,Spending_Score1 ], [ Annual_Income2,Spending_Score2 ], [ Annual_Income3,Spending_Score3], [ Annual_Income4,Spending_Score4 ]])
print(predict)